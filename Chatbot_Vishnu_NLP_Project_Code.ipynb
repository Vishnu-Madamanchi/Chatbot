{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99MuQ6w5ZY9s",
    "outputId": "dfdd28e1-9fca-4a76-c247-131ad4020882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing required packages\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "from keras.layers import Input, Bidirectional,Embedding, Dense, LSTM, TimeDistributed \n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBLj291UZY9v"
   },
   "outputs": [],
   "source": [
    "#Importing the movie_lines and movie_conversations data files\n",
    "movie_lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "movie_conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwqHtgjyZY9v"
   },
   "outputs": [],
   "source": [
    "# Mapping the line id with the text and storing in a dictionary\n",
    "idx_2_line = {}\n",
    "for conversation in movie_lines:\n",
    "    _conversation = conversation.split(' +++$+++ ')\n",
    "    if len(_conversation) == 5:\n",
    "        idx_2_line[_conversation[0]] = _conversation[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLSeXqI8ZY9w"
   },
   "outputs": [],
   "source": [
    "#list of all the conversations and its line id's\n",
    "conversation_list = []\n",
    "for conversation in movie_conv_lines[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conversation_list.append(_conversation.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn5vHxblZY9w",
    "outputId": "084fea8f-c669-4633-966e-cf564a234b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "#Categorizing the text into questions and answers and storing each in different lists\n",
    "conversation_1 = []\n",
    "conversation_2 = []\n",
    "for dialogue in conversation_list:\n",
    "    for i in range(len(dialogue)-1):\n",
    "        conversation_1.append(idx_2_line[dialogue[i]])\n",
    "        conversation_2.append(idx_2_line[dialogue[i+1]])\n",
    "        \n",
    "# Comparing the lengths of conversation_1 and conversation_2\n",
    "print(len(conversation_1))\n",
    "print(len(conversation_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdhYODrEZY9x"
   },
   "outputs": [],
   "source": [
    "#Defining a function to clean the text \n",
    "def clean_sentence(line):\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"i'm\", \"i am\", line)\n",
    "        line = re.sub(r\"he's\", \"he is\", line)\n",
    "        line = re.sub(r\"she's\", \"she is\", line)\n",
    "        line = re.sub(r\"that's\", \"that is\", line)\n",
    "        line = re.sub(r\"what's\", \"what is\", line)\n",
    "        line = re.sub(r\"where's\", \"where is\", line)\n",
    "        line = re.sub(r\"how's\", \"how is\", line)\n",
    "        line = re.sub(r\"\\'ll\", \" will\", line)\n",
    "        line = re.sub(r\"\\'ve\", \" have\", line)\n",
    "        line = re.sub(r\"\\'re\", \" are\", line)\n",
    "        line = re.sub(r\"\\'d\", \" would\", line)\n",
    "        line = re.sub(r\"n't\", \" not\", line)\n",
    "        line = re.sub(r\"won't\", \"will not\", line)\n",
    "        line = re.sub(r\"can't\", \"cannot\", line)\n",
    "        line = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", line)\n",
    "        line = \" \".join(line.split())\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sXixkLxZY9x"
   },
   "outputs": [],
   "source": [
    "# Using the above function to Clean the text\n",
    "clean_conversation_1 = []\n",
    "for dialogue in conversation_1:\n",
    "    clean_conversation_1.append(clean_sentence(dialogue))\n",
    "clean_conversation_2 = []\n",
    "for dialogue in conversation_2:\n",
    "    clean_conversation_2.append(clean_sentence(dialogue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q2C_tEJZY9x",
    "outputId": "9a6fb8bd-ae9d-44e9-9c0d-916230272993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "#Checking the length of the text in each utterance\n",
    "conversation_length = []\n",
    "for dialogue in clean_conversation_1:\n",
    "    conversation_length.append(len(dialogue.split()))\n",
    "for dialogue in clean_conversation_2:\n",
    "    conversation_length.append(len(dialogue.split()))\n",
    "conversation_length = pd.DataFrame(conversation_length, columns=['counts'])\n",
    "print(np.percentile(conversation_length, 80))\n",
    "print(np.percentile(conversation_length, 85))\n",
    "print(np.percentile(conversation_length, 90))\n",
    "print(np.percentile(conversation_length, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12A5Lc-iZY9y",
    "outputId": "5620ceac-79a6-4ae4-e183-b82b22f6b3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138574\n",
      "138574\n"
     ]
    }
   ],
   "source": [
    "# Removing the sentences with less than 2 words or greater than 20 words.\n",
    "conv_min_length = 2\n",
    "conv_max_length = 20\n",
    "\n",
    "conv_1_short_filt = []\n",
    "conv_2_short_filt = []\n",
    "\n",
    "for i, dialogue in enumerate(clean_conversation_1):\n",
    "    if len(dialogue.split()) >= conv_min_length and len(dialogue.split()) <= conv_max_length:\n",
    "        conv_1_short_filt.append(dialogue)\n",
    "        conv_2_short_filt.append(clean_conversation_2[i])\n",
    "        \n",
    "conv_1_short = []\n",
    "conv_2_short = []\n",
    "\n",
    "for i, dialogue in enumerate(conv_2_short_filt):\n",
    "    if len(dialogue.split()) >= conv_min_length and len(dialogue.split()) <= conv_max_length:\n",
    "        conv_2_short.append(dialogue)\n",
    "        conv_1_short.append(conv_1_short_filt[i])\n",
    "        \n",
    "print(len(conv_1_short))\n",
    "print(len(conv_2_short))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gby5r4C6ZY9y",
    "outputId": "1ec03a9c-0166-457f-890a-97d1027642fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you baron\n",
      "not at all countess there's one very good stone in it \n",
      "\n",
      "not at all countess there's one very good stone in it\n",
      "what time is it \n",
      "\n",
      "darling now tell metell me all about yourself who are you\n",
      "you remember the man who walked into the bank of constantinople and walked out uwithu the bank of constantinople \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_no = np.random.randint(1,len(conv_1_short))\n",
    "for i in range(rand_no, rand_no+3):\n",
    "    print(conv_1_short[i])\n",
    "    print(conv_2_short[i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3w7vF64ZY9z",
    "outputId": "3fb0171a-6fe2-4c30-c727-d6076be4957a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  punkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package punkt to /root/nltk_data...\n",
      "      Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading punkt library \n",
    "import nltk\n",
    "nltk.download()\n",
    "#nltk.download(punkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO6TzkFlZY9z"
   },
   "outputs": [],
   "source": [
    "#Number of records to build the model\n",
    "no_records = 100000\n",
    "conv_1_short = conv_1_short[:no_records]\n",
    "conv_2_short = conv_2_short[:no_records]\n",
    "#Tokenizing the words\n",
    "conv_1_short_nltk = [nltk.word_tokenize(dial) for dial in conv_1_short]\n",
    "conv_2_short_nltk = [nltk.word_tokenize(dial) for dial in conv_2_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynpZjXIoZY9z",
    "outputId": "3bbb698f-9f2a-4adc-8858-8504bd50a87c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 80000\n",
      "validation size 20000\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and Validation\n",
    "split_len = len(conv_1_short_nltk)\n",
    "\n",
    "#Using 80% of the data for training\n",
    "data_train  = conv_1_short_nltk[:round(split_len*(80/100))]\n",
    "data_train  = [record[::-1] for record in data_train]\n",
    "data_outcome = conv_2_short_nltk[:round(split_len*(80/100))]\n",
    "\n",
    "#20% of the data for Validation\n",
    "data_valid_inp = conv_1_short_nltk[round(split_len*(80/100)):]\n",
    "data_valid_inp  = [record[::-1] for record in data_valid_inp] \n",
    "validation_output = conv_2_short_nltk[round(split_len*(80/100)):]\n",
    "\n",
    "print('training size', len(data_train))\n",
    "print('validation size', len(data_valid_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es9McxouZY90"
   },
   "outputs": [],
   "source": [
    "# data_vocabulary dictionary will store the freq of each word\n",
    "data_vocabulary = {}\n",
    "for dialogue in conv_1_short_nltk:\n",
    "    for token in dialogue:\n",
    "        if token not in data_vocabulary:\n",
    "            data_vocabulary[token] = 1\n",
    "        else:\n",
    "            data_vocabulary[token] += 1\n",
    "\n",
    "for dialogue in conv_2_short_nltk:\n",
    "    for token in dialogue:\n",
    "        if token not in data_vocabulary:\n",
    "            data_vocabulary[token] = 1\n",
    "        else:\n",
    "            data_vocabulary[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn6SJ7tDZY90"
   },
   "outputs": [],
   "source": [
    "#Removing the words that occur less than 5% of the time in the Vocabulary \n",
    "min_thres_value = 25\n",
    "vocab_count = 0\n",
    "for k,v in data_vocabulary.items():\n",
    "    if v >= min_thres_value:\n",
    "        vocab_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jOJ49gWZY91",
    "outputId": "e211fa65-0c89-40e0-f2e8-d3bbe33aff73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before removing the rare words: 35117\n",
      "Vocab size after removing the rare words: 3168\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size before removing the rare words:\", len(data_vocabulary))\n",
    "print(\"Vocab size after removing the rare words:\", vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7ZCeZ3uZY91"
   },
   "outputs": [],
   "source": [
    "#Assigning a unique number to each word and store it in dictionaries\n",
    "Begining_word_code = 1\n",
    "token_pad = 0\n",
    "\n",
    "token_pos  = 2 \n",
    "dict_enc = {}\n",
    "dict_dec = {1: 'START'}\n",
    "for token, vocab_cnt in data_vocabulary.items():\n",
    "    if vocab_cnt >= min_thres_value: \n",
    "        dict_enc[token] = token_pos\n",
    "        dict_dec[token_pos ] = token\n",
    "        token_pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z7nTuN4ZY91"
   },
   "outputs": [],
   "source": [
    "#Replacing the words which are not present in the dictionary with UNK Tokens\n",
    "dict_dec[len(dict_enc)+2] = '<UNK>'\n",
    "dict_enc['<UNK>'] = len(dict_enc)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ7bwbAiZY91",
    "outputId": "9a2b9dcc-e090-4389-f0f7-d7047c3d20c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3171"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_size = token_pos+1\n",
    "token_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JOrDyQLZY92"
   },
   "outputs": [],
   "source": [
    "#Creating a function that converts the words to vectors\n",
    "def vectorize(dict_enc, word, size_of_vector=20):\n",
    "    transformed_word = np.zeros(shape=(len(word), size_of_vector))\n",
    "    for k in range(len(word)):\n",
    "        for l in range(min(len(word[k]), size_of_vector)):\n",
    "            try:\n",
    "                transformed_word[k][l] = dict_enc[word[k][l]]\n",
    "            except:\n",
    "                transformed_word[k][l] = dict_enc['<UNK>']\n",
    "    return transformed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln17OQuAZY92"
   },
   "outputs": [],
   "source": [
    "length_of_input = 20\n",
    "length_of_output = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km1u89zjZY92",
    "outputId": "06711d84-8b55-45b6-c66c-77ef29dc8119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_enc (80000, 20)\n",
      "train_output_enc (80000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the training data\n",
    "train_input_enc = vectorize(\n",
    "    dict_enc, data_train, size_of_vector=length_of_input)\n",
    "train_output_enc = vectorize(\n",
    "    dict_enc, data_outcome, size_of_vector=length_of_output)\n",
    "\n",
    "print('train_input_enc', train_input_enc.shape)\n",
    "print('train_output_enc', train_output_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSXf97ujZY93",
    "outputId": "043dbc17-6f8d-4820-eda5-d8a56f7df42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_input_enc (20000, 20)\n",
      "valid_output_enc (20000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the validation data\n",
    "valid_input_enc = vectorize(\n",
    "    dict_enc, data_valid_inp, size_of_vector=length_of_input)\n",
    "valid_output_enc = vectorize(\n",
    "    dict_enc, validation_output, size_of_vector=length_of_output)\n",
    "\n",
    "print('valid_input_enc', valid_input_enc.shape)\n",
    "print('valid_output_enc', valid_output_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqhGVBFPZY93"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XQamCu0ZY93"
   },
   "outputs": [],
   "source": [
    "#input and output sizes of the vectors\n",
    "length_of_input = 20\n",
    "length_of_output = 20\n",
    "\n",
    "input_to_enc = Input(shape=(length_of_input,))\n",
    "input_to_dec = Input(shape=(length_of_output,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zxRRHMsZY93",
    "outputId": "cda5d874-7eb1-4bf0-d49a-c93680550fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(None, 20, 512), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice:0\", shape=(None, 512), dtype=float32)\n",
      "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(None, 20, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Defining the encoder and decoder layers\n",
    "from keras.layers import SimpleRNN\n",
    "enc = Embedding(token_size, 128, input_length=length_of_input, mask_zero=True)(input_to_enc)\n",
    "enc = LSTM(512, return_sequences=True, unroll=True)(enc)\n",
    "last_enc = enc[:,-1,:]\n",
    "print('encoder', enc)\n",
    "print('encoder_last', last_enc)\n",
    "\n",
    "dec = Embedding(token_size, 128, input_length=length_of_output, mask_zero=True)(input_to_dec)\n",
    "dec = LSTM(512, return_sequences=True, unroll=True)(dec, initial_state=[last_enc, last_enc])\n",
    "print('decoder', dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5a9UxKQZY93",
    "outputId": "7a215693-f4db-4702-e94e-d81373118dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/truediv:0\", shape=(None, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_2/MatMul:0\", shape=(None, 20, 512), dtype=float32)\n",
      "combined_cxt_dec Tensor(\"concatenate_1/concat:0\", shape=(None, 20, 1024), dtype=float32)\n",
      "result Tensor(\"time_distributed_2/Reshape_1:0\", shape=(None, 20, 3171), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Defining the attention layer\n",
    "from keras.layers import Activation, dot, concatenate\n",
    "atten = dot([dec, enc], axes=[2, 2])\n",
    "atten = Activation('softmax', name='attention')(atten)\n",
    "print('attention', atten)\n",
    "\n",
    "cxt = dot([atten, enc], axes=[2,1])\n",
    "print('context', cxt)\n",
    "\n",
    "combined_cxt_dec = concatenate([cxt, dec])\n",
    "print('combined_cxt_dec', combined_cxt_dec)\n",
    "\n",
    "result = TimeDistributed(Dense(512, activation=\"tanh\"))(combined_cxt_dec)\n",
    "result = TimeDistributed(Dense(token_size, activation=\"softmax\"))(result)\n",
    "print('result', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lStMQ7t3ZY94",
    "outputId": "d7cf1d99-9e90-412e-a56f-feadee8c6683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 128)      405888      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 128)      405888      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 3171)     1626723     time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 5,588,835\n",
      "Trainable params: 5,588,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model summary\n",
    "Att_model = Model(inputs=[input_to_enc, input_to_dec], outputs=[result])\n",
    "Att_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "Att_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0buY5Z2UZY94"
   },
   "outputs": [],
   "source": [
    "training_input_enc = train_input_enc\n",
    "training_input_dec = np.zeros_like(train_output_enc)\n",
    "training_input_dec[:, 1:] = train_output_enc[:,:-1]\n",
    "training_input_dec[:, 0] = Begining_word_code\n",
    "training_output_dec = np.eye(token_size)[train_output_enc.astype('int')]\n",
    "\n",
    "validation_input_enc = valid_input_enc\n",
    "validation_input_dec = np.zeros_like(valid_output_enc)\n",
    "validation_input_dec[:, 1:] = valid_output_enc[:,:-1]\n",
    "validation_input_dec[:, 0] = Begining_word_code\n",
    "validation_output_dec = np.eye(token_size)[valid_output_enc.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzdjalhVZY94",
    "outputId": "fca062db-6c58-42d4-f7d8-27584d98c553"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "80000/80000 [==============================] - 155s 2ms/step - loss: 9.0528e-04 - val_loss: 8.2555e-04\n",
      "Epoch 2/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 7.9726e-04 - val_loss: 7.6897e-04\n",
      "Epoch 3/100\n",
      "80000/80000 [==============================] - 135s 2ms/step - loss: 7.5948e-04 - val_loss: 7.4954e-04\n",
      "Epoch 4/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 7.4162e-04 - val_loss: 7.3780e-04\n",
      "Epoch 5/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 7.2903e-04 - val_loss: 7.2872e-04\n",
      "Epoch 6/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 7.1947e-04 - val_loss: 7.2291e-04\n",
      "Epoch 7/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 7.1111e-04 - val_loss: 7.1771e-04\n",
      "Epoch 8/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 7.0362e-04 - val_loss: 7.1391e-04\n",
      "Epoch 9/100\n",
      "80000/80000 [==============================] - 147s 2ms/step - loss: 6.9681e-04 - val_loss: 7.1089e-04\n",
      "Epoch 10/100\n",
      "80000/80000 [==============================] - 140s 2ms/step - loss: 6.9014e-04 - val_loss: 7.0860e-04\n",
      "Epoch 11/100\n",
      "80000/80000 [==============================] - 148s 2ms/step - loss: 6.8368e-04 - val_loss: 7.0728e-04\n",
      "Epoch 12/100\n",
      "80000/80000 [==============================] - 135s 2ms/step - loss: 6.7738e-04 - val_loss: 7.0511e-04\n",
      "Epoch 13/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.7103e-04 - val_loss: 7.0433e-04\n",
      "Epoch 14/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.6462e-04 - val_loss: 7.0553e-04\n",
      "Epoch 15/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.5803e-04 - val_loss: 7.0553e-04\n",
      "Epoch 16/100\n",
      "80000/80000 [==============================] - 147s 2ms/step - loss: 6.5121e-04 - val_loss: 7.0700e-04\n",
      "Epoch 17/100\n",
      "80000/80000 [==============================] - 147s 2ms/step - loss: 6.4410e-04 - val_loss: 7.0881e-04\n",
      "Epoch 18/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.3685e-04 - val_loss: 7.1078e-04\n",
      "Epoch 19/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.2911e-04 - val_loss: 7.1331e-04\n",
      "Epoch 20/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 6.2109e-04 - val_loss: 7.1663e-04\n",
      "Epoch 21/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 6.1268e-04 - val_loss: 7.1997e-04\n",
      "Epoch 22/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 6.0396e-04 - val_loss: 7.2475e-04\n",
      "Epoch 23/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 5.9496e-04 - val_loss: 7.3033e-04\n",
      "Epoch 24/100\n",
      "80000/80000 [==============================] - 144s 2ms/step - loss: 5.8574e-04 - val_loss: 7.3556e-04\n",
      "Epoch 25/100\n",
      "80000/80000 [==============================] - 147s 2ms/step - loss: 5.7620e-04 - val_loss: 7.4119e-04\n",
      "Epoch 26/100\n",
      "80000/80000 [==============================] - 147s 2ms/step - loss: 5.6670e-04 - val_loss: 7.4860e-04\n",
      "Epoch 27/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 5.5708e-04 - val_loss: 7.5499e-04\n",
      "Epoch 28/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 5.4714e-04 - val_loss: 7.6257e-04\n",
      "Epoch 29/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 5.3743e-04 - val_loss: 7.7107e-04\n",
      "Epoch 30/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 5.2764e-04 - val_loss: 7.7856e-04\n",
      "Epoch 31/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 5.1784e-04 - val_loss: 7.8751e-04\n",
      "Epoch 32/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 5.0816e-04 - val_loss: 7.9657e-04\n",
      "Epoch 33/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 4.9856e-04 - val_loss: 8.0620e-04\n",
      "Epoch 34/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.8918e-04 - val_loss: 8.1523e-04\n",
      "Epoch 35/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.7965e-04 - val_loss: 8.2545e-04\n",
      "Epoch 36/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.7046e-04 - val_loss: 8.3364e-04\n",
      "Epoch 37/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.6129e-04 - val_loss: 8.4355e-04\n",
      "Epoch 38/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.5248e-04 - val_loss: 8.5404e-04\n",
      "Epoch 39/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.4384e-04 - val_loss: 8.6703e-04\n",
      "Epoch 40/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.3518e-04 - val_loss: 8.7404e-04\n",
      "Epoch 41/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.2676e-04 - val_loss: 8.8402e-04\n",
      "Epoch 42/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 4.1826e-04 - val_loss: 8.9417e-04\n",
      "Epoch 43/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 4.1049e-04 - val_loss: 9.0512e-04\n",
      "Epoch 44/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 4.0224e-04 - val_loss: 9.1390e-04\n",
      "Epoch 45/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 3.9399e-04 - val_loss: 9.2530e-04\n",
      "Epoch 46/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.8649e-04 - val_loss: 9.3501e-04\n",
      "Epoch 47/100\n",
      "80000/80000 [==============================] - 135s 2ms/step - loss: 3.7941e-04 - val_loss: 9.4401e-04\n",
      "Epoch 48/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.7198e-04 - val_loss: 9.5517e-04\n",
      "Epoch 49/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 3.6446e-04 - val_loss: 9.6421e-04\n",
      "Epoch 50/100\n",
      "80000/80000 [==============================] - 148s 2ms/step - loss: 3.5699e-04 - val_loss: 9.7463e-04\n",
      "Epoch 51/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 3.5017e-04 - val_loss: 9.8415e-04\n",
      "Epoch 52/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.4322e-04 - val_loss: 9.9447e-04\n",
      "Epoch 53/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.3645e-04 - val_loss: 0.0010\n",
      "Epoch 54/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 3.3022e-04 - val_loss: 0.0010\n",
      "Epoch 55/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.2401e-04 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.1726e-04 - val_loss: 0.0010\n",
      "Epoch 57/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.1111e-04 - val_loss: 0.0010\n",
      "Epoch 58/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 3.0514e-04 - val_loss: 0.0011\n",
      "Epoch 59/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.9867e-04 - val_loss: 0.0011\n",
      "Epoch 60/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.9374e-04 - val_loss: 0.0011\n",
      "Epoch 61/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.8816e-04 - val_loss: 0.0011\n",
      "Epoch 62/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.8254e-04 - val_loss: 0.0011\n",
      "Epoch 63/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.7761e-04 - val_loss: 0.0011\n",
      "Epoch 64/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.7196e-04 - val_loss: 0.0011\n",
      "Epoch 65/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.6803e-04 - val_loss: 0.0011\n",
      "Epoch 66/100\n",
      "80000/80000 [==============================] - 135s 2ms/step - loss: 2.6279e-04 - val_loss: 0.0011\n",
      "Epoch 67/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.5775e-04 - val_loss: 0.0011\n",
      "Epoch 68/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.5359e-04 - val_loss: 0.0011\n",
      "Epoch 69/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.4968e-04 - val_loss: 0.0011\n",
      "Epoch 70/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 2.4587e-04 - val_loss: 0.0012\n",
      "Epoch 71/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.4099e-04 - val_loss: 0.0012\n",
      "Epoch 72/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.3700e-04 - val_loss: 0.0012\n",
      "Epoch 73/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.3391e-04 - val_loss: 0.0012\n",
      "Epoch 74/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 2.3075e-04 - val_loss: 0.0012\n",
      "Epoch 75/100\n",
      "80000/80000 [==============================] - 139s 2ms/step - loss: 2.2699e-04 - val_loss: 0.0012\n",
      "Epoch 76/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.2170e-04 - val_loss: 0.0012\n",
      "Epoch 77/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.1878e-04 - val_loss: 0.0012\n",
      "Epoch 78/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.1560e-04 - val_loss: 0.0012\n",
      "Epoch 79/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.1283e-04 - val_loss: 0.0012\n",
      "Epoch 80/100\n",
      "80000/80000 [==============================] - 137s 2ms/step - loss: 2.1088e-04 - val_loss: 0.0012\n",
      "Epoch 81/100\n",
      "80000/80000 [==============================] - 146s 2ms/step - loss: 2.0837e-04 - val_loss: 0.0012\n",
      "Epoch 82/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 2.0508e-04 - val_loss: 0.0012\n",
      "Epoch 83/100\n",
      "80000/80000 [==============================] - 146s 2ms/step - loss: 2.0169e-04 - val_loss: 0.0012\n",
      "Epoch 84/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 2.0013e-04 - val_loss: 0.0012\n",
      "Epoch 85/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 1.9726e-04 - val_loss: 0.0013\n",
      "Epoch 86/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.9496e-04 - val_loss: 0.0013\n",
      "Epoch 87/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.9243e-04 - val_loss: 0.0013\n",
      "Epoch 88/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.9001e-04 - val_loss: 0.0013\n",
      "Epoch 89/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 1.8777e-04 - val_loss: 0.0013\n",
      "Epoch 90/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 1.8694e-04 - val_loss: 0.0013\n",
      "Epoch 91/100\n",
      "80000/80000 [==============================] - 134s 2ms/step - loss: 1.8409e-04 - val_loss: 0.0013\n",
      "Epoch 92/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.8051e-04 - val_loss: 0.0013\n",
      "Epoch 93/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.8053e-04 - val_loss: 0.0013\n",
      "Epoch 94/100\n",
      "80000/80000 [==============================] - 137s 2ms/step - loss: 1.7866e-04 - val_loss: 0.0013\n",
      "Epoch 95/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.7540e-04 - val_loss: 0.0013\n",
      "Epoch 96/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.7403e-04 - val_loss: 0.0013\n",
      "Epoch 97/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.7244e-04 - val_loss: 0.0013\n",
      "Epoch 98/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.7061e-04 - val_loss: 0.0013\n",
      "Epoch 99/100\n",
      "80000/80000 [==============================] - 138s 2ms/step - loss: 1.6913e-04 - val_loss: 0.0013\n",
      "Epoch 100/100\n",
      "80000/80000 [==============================] - 145s 2ms/step - loss: 1.6805e-04 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/network.py:896: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice:0' shape=(None, 512) dtype=float32>, <tf.Tensor 'strided_slice:0' shape=(None, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "#Running the model\n",
    "Att_model.fit(x=[training_input_enc, training_input_dec], y=[training_output_dec],\n",
    "          validation_data=([validation_input_enc, validation_input_dec], [validation_output_dec]),\n",
    "          batch_size=64, epochs=100)\n",
    "\n",
    "Att_model.save('Att_model_attention_100k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSx-KcD8ZY95"
   },
   "outputs": [],
   "source": [
    "#Defining the prediction and decoding functions\n",
    "def to_predict(input_text):\n",
    "    cleaning_input = clean_sentence(input_text)\n",
    "    token = [nltk.word_tokenize(cleaning_input)]\n",
    "    token = [token[0][::-1]]  #reverseing input seq\n",
    "    inp_enc = vectorize(dict_enc, token, 20)\n",
    "    inp_dec = np.zeros(shape=(len(inp_enc), length_of_output))\n",
    "    inp_dec[:,0] = Begining_word_code\n",
    "    for i in range(1, length_of_output):\n",
    "        output = Att_model.predict([inp_enc, inp_dec]).argmax(axis=2)\n",
    "        inp_dec[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decoding(dict_dec, vector):\n",
    "    output_text = ''\n",
    "    for k in vector:\n",
    "        if k == 0:\n",
    "            break\n",
    "        output_text += ' '\n",
    "        output_text += dict_dec[k]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydiuAhngZY95",
    "outputId": "b5473d2f-3c93-498f-90b8-eadd4c190eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how long does this go on\n",
      "Answer:  i do not know eight or nine\n",
      "Question: please hurry\n",
      "Answer:  i will do anything to help you\n",
      "Question: here in vienna\n",
      "Answer:  all right send for one hour\n",
      "Question: not quite you have not announced our engagement yet\n",
      "Answer:  it must have been <UNK>\n",
      "Question: cut it out pa he'p al with the truck do not fret honey you goin' to be awright\n",
      "Answer:  did he always love to you\n",
      "Question: the first match i got in mind for you is in louisville kentucky\n",
      "Answer:  you do not <UNK> driving orders do you ever\n",
      "Question: i just went there for a job\n",
      "Answer:  then how can we <UNK> this boat\n",
      "Question: we can monitor the cloud's approach and observe the tests from here\n",
      "Answer:  is it safe\n",
      "Question: your lover\n",
      "Answer:  yes i am here\n",
      "Question: the tide came in the tide went out i survived that is the headline i survived\n",
      "Answer:  do not <UNK> me with the details you know how i hate that\n",
      "Question: hmm primitive ventilation\n",
      "Answer:  what exactly\n",
      "Question: have now\n",
      "Answer:  come on\n",
      "Question: i am trying to think when did you start looking so terrible you look awful\n",
      "Answer:  i would know about that really good\n",
      "Question: well listen i know how my boys take ta scrappin' when they take ta drinkin'\n",
      "Answer:  yes mother\n",
      "Question: and dieter\n",
      "Answer:  and her and <UNK> right here that is all i owe you\n",
      "Question: i brought it\n",
      "Answer:  in fact i will just have to ask and questions\n",
      "Question: yeah the camera guy is here\n",
      "Answer:  send him away\n",
      "Question: no you go in there i will have to kill you\n",
      "Answer:  what is going on\n",
      "Question: what does that mean\n",
      "Answer:  you know what i mean\n",
      "Question: new power coins\n",
      "Answer:  on screen\n"
     ]
    }
   ],
   "source": [
    "#Generating answers for random questions\n",
    "for i in range(20):\n",
    "    index_sequence = np.random.randint(1, len(conv_1_short))\n",
    "    out = to_predict(conv_1_short[index_sequence])\n",
    "    print ('Question:', conv_1_short[index_sequence])\n",
    "    print ('Answer:', decoding(dict_dec, out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below \"Q\" is the question, \"A\" is the response/answer from the chatbot and \"Orig A\" is the actual/Original answer for the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUyUBthWZfaW",
    "outputId": "53afefae-b31c-4e80-8aac-9cea3c68d98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: i want you to ride with me somethin' does not sound right\n",
      "A:  i <UNK> your ass with <UNK> <UNK>\n",
      "Orig A: okay lemme clear it with\n",
      "BLEU score:  1.1200407237786664e-231\n",
      "Q: poor larry obedient as ever\n",
      "A:  keep your voice down\n",
      "Orig A: keep your voice down\n",
      "BLEU score:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: you sing and dance\n",
      "A:  a little i just <UNK> from <UNK> film school <UNK> <UNK>\n",
      "Orig A: a little i just graduated from nyu film school cinema studies\n",
      "BLEU score:  0.28997844147152074\n",
      "Q: are you wet\n",
      "A:  just a little wait a minute woman\n",
      "Orig A: just a little wait a minute woman\n",
      "BLEU score:  1.0\n",
      "Q: are you okay\n",
      "A:  yes you\n",
      "Orig A: yes you\n",
      "BLEU score:  1.491668146240062e-154\n",
      "Q: he is very realistic\n",
      "A:  is not the <UNK> <UNK>\n",
      "Orig A: is not the helmet magnificent\n",
      "BLEU score:  6.86809206056511e-78\n",
      "Q: why your mother not breast feed you or something\n",
      "A:  are you asking me if i have something to prove\n",
      "Orig A: are you asking me if i have something to prove\n",
      "BLEU score:  1.0\n",
      "Q: why are ya goin' so early\n",
      "A:  <UNK> i get <UNK> you have <UNK>\n",
      "Orig A: 'cause that is how long it's gonna take baby\n",
      "BLEU score:  0\n",
      "Q: it's been months it's not like him\n",
      "A:  something 's wrong i know it i have heard <UNK> of <UNK> <UNK> south from <UNK>\n",
      "Orig A: something's wrong i know it i have heard rumors of cholera spreading south from hamburg\n",
      "BLEU score:  0.42311785416105785\n",
      "Q: a few words i thought he was imbecile\n",
      "A:  well sir perhaps i should explain\n",
      "Orig A: well sir perhaps i should explain\n",
      "BLEU score:  1.0\n",
      "Q: give it to me\n",
      "A:  what is this supposed to mean\n",
      "Orig A: what is this supposed to mean\n",
      "BLEU score:  1.0\n",
      "Q: i thought you were punishing me\n",
      "A:  why ca not we have <UNK> for you for <UNK>\n",
      "Orig A: why ca not we agree on this\n",
      "BLEU score:  0.2626909894424158\n",
      "Q: i would not put it that way but i love sharks\n",
      "A:  see you later\n",
      "Orig A: you love sharks\n",
      "BLEU score:  1.384292958842266e-231\n",
      "Q: okay you are under arrest now you happy\n",
      "A:  fire department <UNK> do not carry guns\n",
      "Orig A: fire department firemen do not carry guns\n",
      "BLEU score:  0.488923022434901\n",
      "Q: you gotta be kidding what the hell we need that tub for we got fifty million bucks\n",
      "A:  so we get a little more for the boat besides the gold will be <UNK> where it is\n",
      "Orig A: so we get a little more for the boat besides the gold will be safer where it is\n",
      "BLEU score:  0.8394327083733336\n",
      "Q: that is correct\n",
      "A:  and you would not want to <UNK> on the <UNK> of <UNK> data would you\n",
      "Orig A: and you would not want to explode on the basis of false data would you\n",
      "BLEU score:  0.45788313721339824\n",
      "Q: who's the other one\n",
      "A:  it 's supposed to be a <UNK> <UNK>\n",
      "Orig A: white girl named melanie ralston another girlfriend of ordell's\n",
      "BLEU score:  0\n",
      "Q: i was hoping you could tell me\n",
      "A:  you are getting yourself into terrible trouble my son weather 's turning very <UNK> and so am i\n",
      "Orig A: you are getting yourself into terrible trouble my son weather's turning very nasty and so am i\n",
      "BLEU score:  0.6268593335004975\n",
      "Q: you are a comfort to me henry\n",
      "A:  what now writing about it in your <UNK> wo not help\n",
      "Orig A: what now writing about it in your journal wo not help\n",
      "BLEU score:  0.7016879391277372\n",
      "Q: do you hear it right there\n",
      "A:  hear what\n",
      "Orig A: hear what\n",
      "BLEU score:  1.491668146240062e-154\n"
     ]
    }
   ],
   "source": [
    "#Calculating the Bleu Score of each predicted answer.\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "for i in range(20):\n",
    "    index_sequence = np.random.randint(1, len(conv_1_short))\n",
    "    out = to_predict(conv_1_short[index_sequence])\n",
    "    print ('Q:', conv_1_short[index_sequence])\n",
    "    print ('A:', decoding(dict_dec, out[0]))\n",
    "    print ('Orig A:', conv_2_short[index_sequence])\n",
    "    print(\"BLEU score: \",sentence_bleu([conv_2_short[index_sequence].split()],decoding(dict_dec, out[0]).split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First sentence in the output is the question and next sentence is the response from the chatbot to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKEkLlrdZY96",
    "outputId": "5daf57cc-816e-445e-eb9e-d37111a15d7c"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fine i am fine how are you\n"
     ]
    }
   ],
   "source": [
    "#Manual Input of Question. \n",
    "#First sentence in the output is the question and next sentence is the response from the chatbot\n",
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA9C8x9NZY96",
    "outputId": "f029a63a-ec68-4c53-a5cc-4bd5657c9016"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I am good thank you!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " look you are the same\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vB5UzIr6ZY96",
    "outputId": "11acbf5d-be60-4195-8fb1-24e27ac35f94"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Ofcourse I am the same!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you are not\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVQxdugjZY97",
    "outputId": "e0854a6a-6e72-4918-c00c-26830b28591e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " why not?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " because i want to\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQcMErSlZY97",
    "outputId": "9e561e58-4ec0-4adb-f66f-496e7ad662b4"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what you want to?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if i am not being paid to seduce him but he would not kill me you know he did did\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb_XavhdZY97",
    "outputId": "f24beac9-7934-4eea-df51-1bde3f7db34c"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what are you even talking about?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " he was shooting at least she was not <UNK>\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "out = to_predict(input_text)\n",
    "print (decoding(dict_dec, out[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chatbot_Vishnu_Vivek_NLP_Project_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
